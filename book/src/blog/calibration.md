# Calibration
* Scope
* In-process HTTP server

## Scope

One tricky part of planning load test or benchmark is deciding how much 'load'
can be generated by one of the load testing clients, that is how many clients
do we need to set up. Or, most simply, how much load can one client simulate.
Things are complicated by inter-relations between requests/second, request size,
bandwidth, server response timing and server response size.

Our calibration exercise aims to give you a starting point for your
calculations.  That point is an approximation of the neighborhood of the
maximum requests/second that could be generated by the machine running Regatta.

The context, or constraints, are:

* We setup a localhost HTTP server on port `8888`.
* The server responds with `"Hello World!"`.
* The HTTP client makes concurrent requests.

This blog post tracks the evolution of the in-process HTTP server setup.

## In-process HTTP server

The server process is gracfully shutdown when the calibration is completed,
or when the user signals `ctl-c` , or some shut instruction.
We will document the gracefully shutdown elsewhere, so we can set that aside.

Initially a new HTTP server was spawned on each iteration of the Criterion
benchmarks.  Apart from being messy, this generated system wide load that
we expected to interfere with the req/sec throughput estimates.

Once we moved the server setup to be idempotent we saw a reasonable improvement
in throughput.  Criterion reports benchmark results in terms of time.
The average time taken to process 10K requests declined statistically significantly.

[Single server startup](blog/reports/20210822/relative_pdf_small.svg)

That is all we can take from this [full report](blog/reports/20210822).
The reason is the benchmark host is
a developer desktop, which has several background processes running.
Repeating the benchmark runs with and without this change confirmed these
results are indicative of an improvement.

The range of requests/sec is wide: [5k-16k].  This is due to the host system
running unrelated, non-essential processes.

Before pursuing further improvements we establish if the internal calibration
requests/second results are in the ballpark of results reported by 'similarly'
simple server implementations.  We limit ourselves to the Rust based servers
[kcup (v 0.2.1)] and [miniserve (v 0.14.0)] (sample of 100 simulations each):

![Source: http://shiny.chemgrid.org/boxplotr/](images/int-kc-ms-boxplot.svg "Inital boxplot")

Our functionality is simpler than [kcup (v 0.2.1)] and
[miniserve (v 0.14.0)], so we expected higher request/second rates.
We attempted to change the response string into Bytes (static), but that made
no statistically significant improvement and is reverted:

![Source: http://shiny.chemgrid.org/boxplotr/](images/string-bytes-revert-boxplot-openssl.svg "String to Bytes and Back Boxplot")

In the absence of known performance improvements that are low hanging fruit we
decided to explore performance profiling in Rust.
Two guideposts were helpful launch points:

* [Nick Babcock's Guidelines on Benchmarking and Rust]: https://nickb.dev/blog/guidelines-on-benchmarking-and-rust
* [Denis Bakhvalov's Top-Down performance analysis methodology]: https://easyperf.net/blog/2019/02/09/Top-Down-performance-analysis-methodology

### TMAM

### Valgrind

```BASH
RUSTFLAGS="-g" cargo bench --norun --package regatta --bench reqs -- --nocapture
BENCH="./../target/release/reqs"
T_ID="Calibrate/calibrate-limit/10000"
valgrind --tool=callgrind \
         --dump-instr=yes \
         --collect-jumps=yes \
         --simulate-cache=yes \
         $BENCH --bench --profile-time 10 $T_ID
kcachegrind
```

Valgrind showed the HTTPS client is generating many OpenSSL calls.

One of the Rust learning curves is becoming familiar with the state of the Rust
ecosystem - and the valgrind/kcachegrind insight resulted in a decision to
replacing rust-tls with [rustls], via the [hyper-rustls] crate. We discovered
[rustls] has:

* passed a [substantial security audit]
* [outperforms OpenSSL] across
  * [Bulk performance]
  * [Full handshakes]
  * [Resumed handshakes]
  * [Memory usage]
* has substantial adoption efforts via
  * [Linkerd]
  * [Apache (mod_tls)]
  * [Curl]

The medians are internal: 14712.30, kcup: 27973.83, miniserve 10894.32.
All 100 observations are:

![Source: http://shiny.chemgrid.org/boxplotr/](images/string-bytes-revert-boxplot-rustls.svg "Internal(rustls) v KCup v Miniserve Boxplot")

The relative positions are unchanged.  The the medians are higher and dispersion
smaller because the test host was 'quieter'.  Switching to use rustls improved
performance, but not to a level comparable to kcup.

#### Flamegraph

There are several option in this space.  [cargo-flamegraph] and [pprof-rs] and
the pprof-rs integration with Criterion.

Setup:

```BASH
echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
```

```BASH
cargo install flamegraph
flamegraph --no-inline -o reqs_flamegraph.svg ${BENCH}
cargo flamegraph --bench reqs --features calibrate-limit -- --bench
```

#### pprof

The `--profile-time 130` argument is required to generate the pprof output
`profile.pb`.

```bash
cargo bench --bench reqs -- calibrate-limit --nocapture --profile-time 130
go tool pprof -http=:8080 ./../target/criterion/Calibrate/calibrate-limit/10000/profile/profie.pb
```

Possible change:  Use tokio thread local sets for the server setup, and for
the client

* https://github.com/tokio-rs/tokio/issues/2095#issuecomment-573330413
* https://github.com/tokio-rs/tokio/issues/2095#issuecomment-573334953

## Appendix

### Internal v kcup v miniserve

#### Generate requests/second data

```bash
echo Hello World! >/tmp/test-hello
kcup -f /tmp/test-hello &
miniserve /tmp/test-hello -p 5001 &
cargo bench --package regatta --bench reqs -- calibrate-limit --nocapture &>>internal.log;
for i in {1..100} ;do wrk -t12 -c400 -d2s --latency http://127.0.0.1:5000 &>>kcup.log; done
for i in {1..100} ;do wrk -t12 -c400 -d2s --latency http://127.0.0.1:5001 &>>miniserve.log; done
```

#### Collate requests/second data

```bash
echo internal > int.txt
cat internal.log | grep Throughput:|cut -d' ' -f2 >>int.txt
echo kcup >kc.txt
cat kcup.log |grep Requests/sec|cut -d: -f2 >>kc.txt
echo miniserve >ms.txt
cat miniserve.log |grep Requests/sec|cut -d: -f2 >>ms.txt
pr -tm -s, int.txt kc.txt ms.txt >int-kc-ms.csv
rm int.txt kc.txt ms.txt internal.log kcup.log miniserve.log
```

### Internal: String v Bytes

#### Generate requests/second data

Before the change, and after reverting the change

```bash
cargo bench --package regatta --bench reqs -- calibrate-limit --nocapture &>>internal.log;
cargo bench --package regatta --bench reqs -- calibrate-limit --nocapture &>>internal2b.log;
```

After the change

```bash
cargo bench --package regatta --bench reqs -- calibrate-limit --nocapture &>>internal2.log;
```

#### Collate requests/second data

```bash
echo string > int.txt
cat internal.log | grep Throughput:|cut -d' ' -f2 >>int.txt
echo revert > int2b.txt
cat revert.log | grep Throughput:|cut -d' ' -f2 >>int2b.txt
echo bytes > int2.txt
cat bytes.log | grep Throughput:|cut -d' ' -f2 >>int2.txt

pr -tm -s, int.txt int2.txt int2b.txt >int-int2b-int2.csv

rm int.txt int2b.txt int2.txt internal.log internal2b.log internal2.log
```

[flamegraph]: https://github.com/flamegraph-rs/flamegraph
[hyper-rustls]: https://github.com/rustls/hyper-rustls
[kcup (v 0.2.1)]: https://gitlab.com/mrman/kcup-rust
[miniserve (v 0.14.0)]: https://github.com/svenstaro/miniserve
[pprof-rs]: https://github.com/tikv/pprof-rs/blob/master/examples/criterion.rs
[rustls]: https://github.com/rustls/rustls
[Curl]: https://www.abetterinternet.org/post/memory-safe-curl/
[Apache]: https://www.abetterinternet.org/post/memory-safe-tls-apache/
[substantial security audit]: https://github.com/ctz/rustls/blob/master/audit/TLS-01-report.pdf
[outperforms OpenSSL]: https://jbp.io/2019/07/01/rustls-vs-openssl-performance.html
[linkerd]: https://github.com/linkerd/linkerd2
[Bulk performance]: https://jbp.io/2019/07/02/rustls-vs-openssl-bulk-performance.html
[Full handshakes]: https://jbp.io/2019/07/02/rustls-vs-openssl-handshake-performance.html
[Resumed handshakes]: https://jbp.io/2019/07/02/rustls-vs-openssl-resumption-performance.html
[Memory usage]: https://jbp.io/2019/07/02/rustls-vs-openssl-memory-usage.html
