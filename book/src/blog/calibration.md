# Calibration
* Scope
* In-process HTTP server

## Scope

One tricky part of planning load test or benchmark is deciding how much 'load'
can be generated by one of the load testing clients, that is how many clients
do we need to set up. Or, most simply, how much load can one client simulate.
Things are complicated by inter-relations between requests/second, request size,
bandwidth, server response timing and server response size.

Our calibration exercise aims to give you a starting point for your
calculations.  That point is an approximation of the neighborhood of the
maximum requests/second that could be generated by the machine running Regatta.

The context, or constraints, are:

* We setup a localhost HTTP server on port `8888`.
* The server responds with `"Hello World!"`.
* The HTTP client makes concurrent requests.

This blog post tracks the evolution of the in-process HTTP server setup.

## In-process HTTP server

The server process is gracfully shutdown when the calibration is completed,
or when the user signals `ctl-c` , or some shut instruction.
We will document the gracefully shutdown elsewhere, so we can set that aside.

Initially a new HTTP server was spawned on each iteration of the Criterion
benchmarks.  Apart from being messy, this generated system wide load that
we expected to interfere with the req/sec throughput estimates.

Once we moved the server setup to be idempotent we saw a reasonable improvement
in throughput.  Criterion reports benchmark results in terms of time.
The average time taken to process 10K requests declined statistically significantly.

[Single server startup](blog/reports/20210822/relative_pdf_small.svg)

That is all we can take from this [full report](blog/reports/20210822).
The reason is the benchmark host is
a developer desktop, which has several background processes running.
Repeating the benchmark runs with and without this change confirmed these
results are indicative of an improvement.

The range of requests/sec is wide: [5k-16k].  This is due to the host system
running unrelated, non-essential processes.

Before pursuing further improvements we establish if the internal calibration
requests/second results are in the ballpark of results reported by 'similarly'
simple server implementations.  We limit ourselves to the Rust based servers
[kcup (v 0.2.1)] and [miniserve (v 0.14.0)] (sample of 100 simulations each):

![Source: http://shiny.chemgrid.org/boxplotr/](images/
internal-kcup-miniserve-boxplot-rustls-uncongested-boxplot-openssl-congested.svg "Inital boxplot")

Our functionality is simpler than [kcup (v 0.2.1)] and
[miniserve (v 0.14.0)], so we expected higher request/second rates.
We attempted to change the response string into Bytes (static), but that made
no statistically significant improvement and was reverted:

![Source: http://shiny.chemgrid.org/boxplotr/](images/string-bytes-revert-boxplot-openssl.svg "String to Bytes and Back Boxplot")

In the absence of known, easy performance improvements we decided to explore performance profiling in Rust.
Two guideposts were helpful launch points:

* [Nick Babcock's Guidelines on Benchmarking and Rust]: https://nickb.dev/blog/guidelines-on-benchmarking-and-rust
* [Denis Bakhvalov's Top-Down performance analysis methodology]: https://easyperf.net/blog/2019/02/09/Top-Down-performance-analysis-methodology

### TMAM

### Valgrind

```BASH
RUSTFLAGS="-g" cargo bench --norun --package regatta --bench reqs -- --nocapture
BENCH="./../target/release/reqs"
T_ID="Calibrate/calibrate-limit/10000"
valgrind --tool=callgrind \
         --dump-instr=yes \
         --collect-jumps=yes \
         --simulate-cache=yes \
         $BENCH --bench --profile-time 10 $T_ID
kcachegrind
```

Valgrind showed the HTTPS client is generating many calls related to OpenSSL.

When adopting Rust, one of the learning curves is becoming familiar with the
state of the Rust ecosystem - and the valgrind/kcachegrind insight resulted in
a decision to replace the `rust-tls` crate with [rustls], via the
[hyper-rustls] crate. We learned [rustls] has:

* passed a [substantial security audit]
* [outperforms OpenSSL] across
  * [Bulk performance]
  * [Full handshakes]
  * [Resumed handshakes]
  * [Memory usage]
* has substantial adoption efforts via
  * [Linkerd]
  * [Apache (mod_tls)]
  * [Curl]

After rewriting the client code to use [rustls], the median requests/second
are internal: 14,712.30, kcup: 27,973.83, miniserve 10,894.32.
All 300 observations are summarized in these box plots (uncongested test desktop):

![Source: http://shiny.chemgrid.org/boxplotr/](images/internal-kcup-miniserve-boxplot-rustls-uncongested.svg "Internal(rustls) v KCup v Miniserve Boxplot")

The relative positions are unchanged.  The the medians are higher and dispersion
smaller because the test host was 'quieter'.  We know from the [rustls] benchmarks we
linked to, that switching to use [rustls] improved performance, but not to a
level comparable to [kcup].

#### Flamegraphs

There are several option in this space.  [cargo-flamegraph] and [pprof-rs] and
the pprof-rs integration with Criterion.

Setup:

```BASH
echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid
```

###### cargo-flamegraph

We include these instructions for completeness.  The results weren't
particularly useful.  Meaning we couldn't interactively explore them, and we
didn't find a way to use its output with other tools.  We didn't look too hard
after we discovered pprof could be used with Criterion benchmarks (next),
so please let us know if we have overlooked anything significant.

```BASH
cargo install flamegraph
flamegraph --no-inline -o reqs_flamegraph.svg ${BENCH}
cargo flamegraph --bench reqs --features calibrate-limit -- --bench
```

#### pprof (Cargo Criterion)

The pprof crate can generate profiles from Criterion benchmarks, and the output
data can be explored with the tooling developed around Go, `go tool pprof ...`
as well as the [SpeedScope] project.
These two features won us over.

The criterion option, `--profile-time 130`, is required to generate the pprof output
`profile.pb`.

```bash
cargo bench --bench reqs -- calibrate-limit --nocapture --profile-time 130
go tool pprof -http=:8080 ./../target/criterion/Calibrate/calibrate-limit/10000/profile/profie.pb
go tool pprof -svg profile300.gz ./../target/criterion/Calibrate/calibrate-limit/10000/profile/profie.pb
```

Possible change:  Use tokio thread local sets for the server setup, and for
the client

* https://github.com/tokio-rs/tokio/issues/2095#issuecomment-573330413
* https://github.com/tokio-rs/tokio/issues/2095#issuecomment-573334953

## Appendix

### Internal v kcup v miniserve

#### Generate requests/second data

```bash
echo Hello World! >/tmp/test-hello
kcup -f /tmp/test-hello &
miniserve /tmp/test-hello -p 5001 &
cargo bench --package regatta --bench reqs -- calibrate-limit --nocapture &>>internal.log;
for i in {1..100} ;do wrk -t12 -c400 -d2s --latency http://127.0.0.1:5000 &>>kcup.log; done
for i in {1..100} ;do wrk -t12 -c400 -d2s --latency http://127.0.0.1:5001 &>>miniserve.log; done
```

#### Collate requests/second data

```bash
echo internal > int.txt
cat internal.log | grep Throughput:|cut -d' ' -f2 >>int.txt
echo kcup >kc.txt
cat kcup.log |grep Requests/sec|cut -d: -f2 >>kc.txt
echo miniserve >ms.txt
cat miniserve.log |grep Requests/sec|cut -d: -f2 >>ms.txt
pr -tm -s, int.txt kc.txt ms.txt >int-kc-ms.csv
rm int.txt kc.txt ms.txt internal.log kcup.log miniserve.log
```

### Internal: String v Bytes

#### Generate requests/second data

Before the change, and after reverting the change

```bash
cargo bench --package regatta --bench reqs -- calibrate-limit --nocapture &>>internal.log;
cargo bench --package regatta --bench reqs -- calibrate-limit --nocapture &>>internal2b.log;
```

After the change

```bash
cargo bench --package regatta --bench reqs -- calibrate-limit --nocapture &>>internal2.log;
```

#### Collate requests/second data

```bash
echo string > int.txt
cat internal.log | grep Throughput:|cut -d' ' -f2 >>int.txt
echo revert > int2b.txt
cat revert.log | grep Throughput:|cut -d' ' -f2 >>int2b.txt
echo bytes > int2.txt
cat bytes.log | grep Throughput:|cut -d' ' -f2 >>int2.txt

pr -tm -s, int.txt int2.txt int2b.txt >int-int2b-int2.csv

rm int.txt int2b.txt int2.txt internal.log internal2b.log internal2.log
```

[flamegraph]: https://github.com/flamegraph-rs/flamegraph
[hyper-rustls]: https://github.com/rustls/hyper-rustls
[kcup (v 0.2.1)]: https://gitlab.com/mrman/kcup-rust
[miniserve (v 0.14.0)]: https://github.com/svenstaro/miniserve
[pprof-rs]: https://github.com/tikv/pprof-rs/blob/master/examples/criterion.rs
[rustls]: https://github.com/rustls/rustls
[Curl]: https://www.abetterinternet.org/post/memory-safe-curl/
[Apache]: https://www.abetterinternet.org/post/memory-safe-tls-apache/
[substantial security audit]: https://github.com/ctz/rustls/blob/master/audit/TLS-01-report.pdf
[outperforms OpenSSL]: https://jbp.io/2019/07/01/rustls-vs-openssl-performance.html
[linkerd]: https://github.com/linkerd/linkerd2
[Bulk performance]: https://jbp.io/2019/07/02/rustls-vs-openssl-bulk-performance.html
[Full handshakes]: https://jbp.io/2019/07/02/rustls-vs-openssl-handshake-performance.html
[Resumed handshakes]: https://jbp.io/2019/07/02/rustls-vs-openssl-resumption-performance.html
[Memory usage]: https://jbp.io/2019/07/02/rustls-vs-openssl-memory-usage.html
[speedscope]: https://www.speedscope.app/
